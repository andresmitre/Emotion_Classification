	{
	 "cells": [
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Emotion Classification\n",
		"**Module 3: Data acquisition**\n",
		"* Author: [Andrés Mitre](https://github.com/andresmitre), [Center for Research in Mathematics (CIMAT)](http://www.cimat.mx/en) in Zacatecas, México.\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Module   |Title\n",
		"---------|--------------\n",
		"Module 1 |[Introduction](https://github.com/andresmitre/Emotion_Classification/blob/master/introduction.ipynb)\n",
		"Module 2 |[Haar Cascade Algorithm](https://github.com/andresmitre/Emotion_Classification/blob/master/Haar_Feature_based_Cascade_Classifiers.ipynb)\n",
		"Module 3 |[Data acquisition](https://github.com/andresmitre/Emotion_Classification/blob/master/data_acquisition.ipynb)\n",
		"Module 4 |[Regression Neural Network](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class4.ipynb)\n",
		"Module 5 |[Time Series Neural Network](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb)\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Goals\n",
		"\n",
		"*  Capture RAW data from FER and GSR.\n",
		"*  Export data to a CSV file.\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Raw Data\n",
		"\n",
		"The data collection, starts with a FER(Facial Expression Recognition) using [haarcascades](https://github.com/opencv/opencv), with a professional camera/webcam. Images were taken while 23 participants watched a series of films related with emotions. The images were saved into separate emotions folders. GSR lectures were recorded as well since stress and boredom emotions stimulates the activity of sweet glands. the GSR were captured with the [Grove - GSR sensor](https://www.seeedstudio.com/Grove-GSR-sensor-p-1614.html) and saved into a CSV file. \n",
		"\n",
		"**Galvanic Skin Response (GSR)**\n",
		"\n",
		"The Galvanic Skin Response (GSR) is defined as a change in the electrical properties of the skin. The signal can be used for capturing the autonomic nerve responses as a parameter of the sweat gland function. The measurement is relatively simple, and has a good repeatability. Therefore the GSR measurement can be considered to be a simple and useful tool for examination of the autonomous nervous system function, and especially the peripheral sympathetic system.\n",
		"\n",
		"For detailed information I highly reccomend to check [The complete pocket guide by IMOTIONS](https://imotions.com/guides/).\n",
		"\n",
		"* Materials required:\n",
		"    * **Camera** - [EOS Rebel T3 – Canon Profesional.](https://www.amazon.com/Canon-Digital-18-55mm-discontinued-manufacturer/dp/B004J3Y9U6).\n",
		"    * **Monitor** - [LCD 22 in, TFT FPD2275W-MX](https://www.cnet.com/products/gateway-fpd2275w/specs/).\n",
		"    * **Keyboard** - DELL SK-8115.\n",
		"    * **Speakers** - Subwoofer DELL A525.\n",
		"    * **GSR Sensor** - [Grove - GSR Sensor](https://www.seeedstudio.com/Grove-GSR-sensor-p-1614.html).\n",
		"    * **Computer** - [Dell Inspiron 13-7359 Signature Edition](http://www.dell.com/en-us/shop/dell-laptops/new-inspiron-13-7000-series-2-in-1-laptop/spd/inspiron-13-7359-laptop).\n",
		"* Films Stimulants:\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Emotion   |Film                |Director\n",
		"---------|---------------------|--------\n",
		"Neutral  |The Lover            |Jean-Jacques Annaud\n",
		"Happy    |When Harry Met Sally |Rob Reiner\n",
		"Stress   |Irreversible         |Gaspar Noé\n",
		"Boredom  |Amateur film         |[Merrifield, C. and Danckert, J.](https://www.youtube.com/watch?v=s34zGmq3rXQ)\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Procedure\n",
		"The emotions from the above table were validated by more than 100 participants in the documentation project [[contact Author]](https://github.com/andresmitre). the collection of raw data takes place in a lapse of 30 seconds in every emotion. The Raw data was collected in the next sequence: \n",
	    "\n",
		"* Sequence:\n",
	    "    * **Neutral > Stress > Neutral > Boredom > Neutral > Happy**.\n",
	    "\n",
	    "The photos were taken at 10FPS. a total of 1380 images were selected as the training data set. where 10 images per emotion of every participant were captured. The images were selected according to the biggest difference between the emotion and the neutral.\n",
		"\n",
		"![Experiment](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/experiment.png \"Experiment\")\n",
	    "\n",
	    "Participant during the experiment.\n",
	    "\n",
	    "##CSV Files\n",
        "The raw data was collected into a CSV file. Each participant had their each CSV file for every emotion. Time, ID, Emotion and Picture number were the fields collected into de CSV, to identify the corresponding image to the specific time. The next figure represents an example of the data collected coresponding to the participant 3 (S3).\n",
        "\n",
		"![csvfile](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/csv.png \"CSV File\")\n",
		"\n"
		]
	  },
	   {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Now we find the faces in the image. If faces are found, it returns the positions of detected faces as Rect(x,y,w,h). Once we get these locations, we can create a ROI for the face and apply eye detection on this ROI (since eyes are always on the face !!! ).\n",
		"\n"   
	   ]
	  },
	  {
	   "cell_type": "code",
	   "execution_count": 2,
	   "metadata": {},
	   "source": [
		"faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
		"for (x,y,w,h) in faces:\n",
		"    cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
		"    roi_gray = gray[y:y+h, x:x+w]\n",
		"    roi_color = img[y:y+h, x:x+w]\n",
		"    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
		"    for (ex,ey,ew,eh) in eyes:\n",
		"        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
		"cv.imshow('img',img)\n",
		"cv.waitKey(0)\n",
		"cv.destroyAllWindows()"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
	   "## Additional Resources\n",
	   "\n",
	   "Video Lecture on [Face Detection and Tracking](https://www.youtube.com/watch?v=WfdYYNamHZ8).\n",
	   "\n"  
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"In order to use different classifiers for face, eyes, smiles and upper body. OpenCV has several XML files, try some of these [here](https://github.com/opencv/opencv/tree/master/data/haarcascades).\n",
		"\n"   
	   ]
	  }
	 ],
	 "metadata": {
	  "anaconda-cloud": {},
	  "kernelspec": {
	   "display_name": "Python 3",
	   "language": "python",
	   "name": "python3"
	  },
	  "language_info": {
	   "codemirror_mode": {
		"name": "ipython",
		"version": 3
	   },
	   "file_extension": ''.py",
	   "mimetype": "text/x-python",
	   "name": "python",
	   "nbconvert_exporter": "python",
	   "pygments_lexer": "ipython3",
	   "version": "3.6.3"
	  }
	 },
	 "nbformat": 4,
	 "nbformat_minor": 1
	}

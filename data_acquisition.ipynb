	{
	 "cells": [
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Emotion Classification\n",
		"**Module 3: Data acquisition**\n",
		"* Author: [Andrés Mitre](https://github.com/andresmitre), [Center for Research in Mathematics (CIMAT)](http://www.cimat.mx/en) in Zacatecas, México.\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Module   |Title\n",
		"---------|--------------\n",
		"Module 1 |[Introduction](https://github.com/andresmitre/Emotion_Classification/blob/master/introduction.ipynb)\n",
		"Module 2 |[Haar Cascade Algorithm](https://github.com/andresmitre/Emotion_Classification/blob/master/Haar_Feature_based_Cascade_Classifiers.ipynb)\n",
		"Module 3 |[Data acquisition](https://github.com/andresmitre/Emotion_Classification/blob/master/data_acquisition.ipynb)\n",
		"Module 4 |[Regression Neural Network](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class4.ipynb)\n",
		"Module 5 |[Time Series Neural Network](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb)\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Goals\n",
		"\n",
		"*  Capture RAW data from FER and GSR.\n",
		"*  Export data to a CSV file.\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Emotion   |Film                |Director\n",
		"---------|---------------------|--------\n",
		"Neutral  |The Lover            |Jean-Jacques Annaud\n",
		"Happy    |When Harry Met Sally |Rob Reiner\n",
		"Stress   |Irreversible         |Gaspar Noé\n",
		"Boredom  |Amateur film         |[Merrifield, C. and Danckert, J.](https://www.youtube.com/watch?v=s34zGmq3rXQ)\n"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"# Procedure\n",
		"The emotions from the above table were validated by more than 100 participants in the documentation project [[contact Author]](https://github.com/andresmitre). the collection of raw data takes place in a lapse of 30 seconds in every emotion. The Raw data was collected in the next sequence: \n",
	    "\n",
		"* Sequence:\n",
	    "    * **Neutral > Stress > Neutral > Boredom > Neutral > Happy**.\n",
	    "\n",
	    "The photos were taken at 10FPS. a total of 1380 images were selected as the training data set. where 10 images per emotion of every participant were captured. The images were selected according to the biggest difference between the emotion and the neutral.\n",
		"\n",
		"![Experiment](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/experiment.png \"Experiment\")\n",
	    "\n",
	    "Participant during the experiment.\n",
	    "\n",
	    "##CSV Files\n",
        "The raw data was collected into a CSV file. Each participant had their each CSV file for every emotion. Time, ID, Emotion and Picture number were the fields collected into de CSV, to identify the corresponding image to the specific time. The next figure represents an example of the data collected coresponding to the participant 3 (S3).\n",
        "\n",
		"![csvfile](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/csv.png \"CSV File\")\n",
		"\n"
		]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
	    "##FER Dataset\n",
        "The Facial Expression Recognition images were saved into seperate directories (named with the emotion) every single image were recorded in the next format:\n",
        "\n",
		"* **Emotion + ID of the participant + Picture Number**.\n",
		"\n",
		"![FER Data set](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/data_set.png \"FER data set\")\n",
		"\n",
		"![Facial_db](https://raw.githubusercontent.com/andresmitre/Emotion_Classification/master/Images/facial_db.png \"Facial db\")\n",
		"\n"				
		]
	  },
	   {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"Now we find the faces in the image. If faces are found, it returns the positions of detected faces as Rect(x,y,w,h). Once we get these locations, we can create a ROI for the face and apply eye detection on this ROI (since eyes are always on the face !!! ).\n",
		"\n"   
	   ]
	  },
	  {
	   "cell_type": "code",
	   "execution_count": 2,
	   "metadata": {},
	   "source": [
		"faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
		"for (x,y,w,h) in faces:\n",
		"    cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
		"    roi_gray = gray[y:y+h, x:x+w]\n",
		"    roi_color = img[y:y+h, x:x+w]\n",
		"    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
		"    for (ex,ey,ew,eh) in eyes:\n",
		"        cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
		"cv.imshow('img',img)\n",
		"cv.waitKey(0)\n",
		"cv.destroyAllWindows()"
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
	   "## Additional Resources\n",
	   "\n",
	   "Video Lecture on [Face Detection and Tracking](https://www.youtube.com/watch?v=WfdYYNamHZ8).\n",
	   "\n"  
	   ]
	  },
	  {
	   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
		"In order to use different classifiers for face, eyes, smiles and upper body. OpenCV has several XML files, try some of these [here](https://github.com/opencv/opencv/tree/master/data/haarcascades).\n",
		"\n"   
	   ]
	  }
	 ],
	 "metadata": {
	  "anaconda-cloud": {},
	  "kernelspec": {
	   "display_name": "Python 3",
	   "language": "python",
	   "name": "python3"
	  },
	  "language_info": {
	   "codemirror_mode": {
		"name": "ipython",
		"version": 3
	   },
	   "file_extension": ''.py",
	   "mimetype": "text/x-python",
	   "name": "python",
	   "nbconvert_exporter": "python",
	   "pygments_lexer": "ipython3",
	   "version": "3.6.3"
	  }
	 },
	 "nbformat": 4,
	 "nbformat_minor": 1
	}
